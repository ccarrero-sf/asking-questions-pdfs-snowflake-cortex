{"cells":[{"cell_type":"markdown","id":"9960b570-1bb7-4a38-a4fe-6927337dba45","metadata":{"collapsed":false,"name":"CELL1"},"source":["# Demo: Asking questions to your own Documents Using Snowflake Cortex\n","This notebook demo the usage of new Cortex-LLM functions. Using UDTFs (User Defined Table Functions), PDF documents are read and chunked. Embeddings are used to create vectors for each chunk using one of Snowflake Cortex functions. Those vectors are used later to find similarities with the questions. The user can choose to use RAG to answer questions, so those chunks are provided as context in the prompt.\n","\n","Streams and Tasks are also used. Each time a new PDF is uploaded into the stage area, it is automatically processed and embeddings are created and appended to a Snowflake table containing the text chunks, their vector array for later use.\n","\n","Streamlit in Snowflake app is also used to provide a nifty interface to ask your questions, monitor the new stream of documents as you upload them to the stage behind the scenes, and ask a given LLama2 model to use those docs as context."]},{"cell_type":"code","execution_count":null,"id":"3775908f-ca36-4846-8f38-5adca39217f2","metadata":{"language":"python","name":"Connect"},"outputs":[],"source":["#  Copyright (c) 2023 Snowflake Computing Inc. All rights reserved.\n","\n","# Import python packages\n","# import streamlit as st\n","import pandas as pd\n","import json"]},{"cell_type":"markdown","id":"84cc9c9a","metadata":{},"source":["## Creating Python functions for local use as well as for creating UDTF"]},{"cell_type":"code","execution_count":null,"id":"a60e9cc4","metadata":{},"outputs":[],"source":["def get_new_session(creds_file):\n","    with open(creds_file, 'r') as ff:\n","        conn_param=json.load(ff)\n","\n","    conn_param\n","    return Session.builder.configs(conn_param).create() "]},{"cell_type":"markdown","id":"78cc47f1","metadata":{},"source":["Note! The below cell will require you to add _langchain_ and _PyPDF2_ packages to be installed first. Look at the README.md for this demo to understand how."]},{"cell_type":"code","execution_count":null,"id":"d33198fa-7e53-41e8-9b49-47e19ad282f5","metadata":{"language":"python","name":"functions_to_read_chunk"},"outputs":[],"source":["#A class for chunking text and returning a table via UDTF\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from snowflake.snowpark.files import SnowflakeFile\n","import PyPDF2, io\n","import logging\n","\n","class pdf_text_chunker:\n","\n","    def read_pdf(self, file_url: str) -> str:\n","    \n","        logger = logging.getLogger(\"udf_logger\")\n","        logger.info(f\"Opening file {file_url}\")\n","    \n","        with SnowflakeFile.open(file_url, 'rb') as f:\n","            buffer = io.BytesIO(f.readall())\n","            \n","        reader = PyPDF2.PdfReader(buffer)   \n","        text = \"\"\n","        for page in reader.pages:\n","            try:\n","                text += page.extract_text().replace('\\n', ' ')\n","            except:\n","                text = \"Unable to Extract\"\n","                logger.warn(f\"Unable to extract from file {file_url}, page {page}\")\n","        \n","        return text\n","\n","    def process(self,file_url: str):\n","\n","        text = self.read_pdf(file_url)\n","        \n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size = 3500, #Adjust this as you see fit\n","            chunk_overlap  = 200, #This let's text have some form of overlap. Useful for keeping chunks contextual\n","            length_function = len\n","        )\n","    \n","        chunks = text_splitter.split_text(text)\n","        df = pd.DataFrame(chunks, columns=['chunks'])\n","        \n","        yield from df.itertuples(index=False, name=None)"]},{"cell_type":"markdown","id":"914abcfa","metadata":{},"source":["Let's start by grabbing an active Snowpark session or create a new one"]},{"cell_type":"code","execution_count":null,"id":"2c0847f9","metadata":{},"outputs":[],"source":["# We can also use Snowpark for our analyses!\n","from snowflake.snowpark.session import Session\n","from snowflake.snowpark.context import get_active_session\n","from snowflake.snowpark.exceptions import SnowparkSessionException\n","\n","from_snowflake_notebooks=False\n","try:\n","    session = get_active_session()\n","    from_snowflake_notebooks=True\n","except SnowparkSessionException as sse:\n","    session = get_new_session('./creds_clakkad_aws_uswest2.json')"]},{"cell_type":"markdown","id":"28a43612","metadata":{},"source":["Confirm that the account picked by the session is correct"]},{"cell_type":"code","execution_count":null,"id":"05c47733","metadata":{},"outputs":[],"source":["session.get_current_account()"]},{"cell_type":"markdown","id":"c43416e8","metadata":{},"source":["We'll now create all the Database & Schema level objects "]},{"cell_type":"code","execution_count":null,"id":"5e386336","metadata":{},"outputs":[],"source":["database=\"DEMO_DB\"\n","schema=\"CORTEX\"\n","docs_stage=\"DOCS\"\n","udf_stage=\"UDF\"\n","app_stage=\"SIS_APPS\"\n","docs_stream=\"DOCS_STREAM\"\n","chunk_table=\"UNSTRUCTURED_DOCS_WITH_CHUNKS\"\n","task_name=\"TASK_EXTRACT_CHUNK_VEC_FROM_PDF\""]},{"cell_type":"code","execution_count":null,"id":"38a17d8d","metadata":{},"outputs":[],"source":["session.sql(f\"CREATE DATABASE IF NOT EXISTS {database}\").collect()\n","session.sql(f\"CREATE SCHEMA IF NOT EXISTS {database}.{schema}\").collect()\n","\n","\n","if not from_snowflake_notebooks:\n","    session.use_database(database)\n","    session.use_schema(schema)"]},{"cell_type":"code","execution_count":null,"id":"16c2a3c7","metadata":{},"outputs":[],"source":["session.sql(f\"\"\"\n","            CREATE OR REPLACE STAGE {docs_stage} \n","            ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n","            DIRECTORY = (ENABLE = TRUE)\"\"\"\n","        ).collect()\n","\n","docs_available = session.sql(f\"\"\"\n","                            SELECT \n","                            RELATIVE_PATH\n","                            ,BUILD_SCOPED_FILE_URL(@{docs_stage}, RELATIVE_PATH) \n","                            FROM DIRECTORY(@{docs_stage})\"\"\"\n","                        ).collect()\n","\n","docs_available\n"]},{"cell_type":"markdown","id":"2c02fbc9-5c4b-4751-b4ba-e93bd230658a","metadata":{"collapsed":false,"name":"CELL8"},"source":["Now we create a stream on the DOCS staging area. This stream will track all new documents uploaded into that staging area."]},{"cell_type":"code","execution_count":null,"id":"51e17718-2982-4114-9c7f-a85be3c97bec","metadata":{"language":"python","name":"list_current_pdfs"},"outputs":[],"source":["session.sql(f\"\"\" CREATE OR REPLACE STREAM {docs_stream} ON STAGE {docs_stage} \"\"\").collect()\n","\n","docs_available_in_stream = session.sql(f\"\"\" \n","            SELECT *,\n","            BUILD_SCOPED_FILE_URL(@{docs_stage}, RELATIVE_PATH) AS SCOPED_FILE_URL \n","            FROM {docs_stream} WHERE METADATA$ACTION = 'INSERT'\n","            LIMIT 1\n","        \"\"\").collect()\n","\n","docs_available_in_stream"]},{"cell_type":"markdown","id":"fd6098a0","metadata":{},"source":["Creating a table to store the links to PDF docs we'll process along with their text splits & vectors"]},{"cell_type":"code","execution_count":null,"id":"0a23618b","metadata":{},"outputs":[],"source":["session.sql(f\"\"\" \n","            CREATE OR REPLACE TABLE {chunk_table} (\n","            RELATIVE_PATH VARCHAR,\n","            FILE_URL VARCHAR,\n","            SCOPED_FILE_URL VARCHAR,\n","            CHUNK VARCHAR,\n","            TEXT_CHUNK_VECTOR VECTOR(FLOAT, 768)\n","            )\n","        \"\"\").collect()"]},{"cell_type":"markdown","id":"23cc9d5d","metadata":{},"source":["Register the Python function 'pdf_text_chunker' as a UDTF. We're also going to declare any packages used within the Python code for this function and which is available from the Snowflake Anaconda channel."]},{"cell_type":"code","execution_count":null,"id":"6d0958c8-62a4-46d0-8b56-a96dd718d1ba","metadata":{"codeCollapsed":false,"language":"python","name":"register_udf"},"outputs":[],"source":["from snowflake.snowpark.types import StringType, StructField, StructType\n","\n","session.sql(f\"\"\"\n","            CREATE OR REPLACE STAGE {udf_stage} \n","            ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n","            DIRECTORY = (ENABLE = TRUE)\"\"\"\n","        ).collect()\n","\n","\n","schema = StructType([\n","     StructField(\"chunk\", StringType())\n"," ])\n","\n","session.udtf.register( \n","    pdf_text_chunker,\n","    output_schema= schema, \n","    input_types = [StringType()] , \n","    is_permanent = True , \n","    name = 'pdf_text_chunker' , \n","    replace = True , \n","    packages=['snowflake-snowpark-python', 'pypdf2','pandas','langchain'], \n","    stage_location = udf_stage\n",")"]},{"cell_type":"markdown","id":"69d752aa-29db-4e1f-9abc-5d3b5ff537fc","metadata":{"collapsed":false,"name":"CELL3"},"source":["Process the current PDFs in the DOCS staging area. We are using an internal staging area for simplicity but this would be an S3 in the CSP.\n","\n","Here we call the UDTF defined before to process the documents and create the new table:"]},{"cell_type":"code","execution_count":null,"id":"5769d7f6-dc1f-4ca2-bf34-976c7cdf5073","metadata":{"language":"sql","name":"process_documents"},"outputs":[],"source":["text_chunks_new_doc = session.sql(f\"\"\"\n","            SELECT \n","                RELATIVE_PATH\n","                ,FILE_URL\n","                ,BUILD_SCOPED_FILE_URL(@{docs_stage}, RELATIVE_PATH) AS SCOPED_FILE_URL\n","                ,FUNC.CHUNK AS CHUNK\n","            FROM \n","                {docs_stream}, \n","                TABLE(PDF_TEXT_CHUNKER(BUILD_SCOPED_FILE_URL(@{docs_stage}, RELATIVE_PATH))) AS FUNC\n","            WHERE METADATA$ACTION = 'INSERT'\n","            LIMIT 1\n","        \"\"\").collect()\n","\n","text_chunks_new_doc"]},{"cell_type":"markdown","id":"9cb47487-5485-4996-863f-f77bb3d7009b","metadata":{"collapsed":false,"name":"CELL4"},"source":["Let´s do some testing to check it finds documents related to the question we are asking:"]},{"cell_type":"code","execution_count":null,"id":"d45bc2f0-8379-4f50-af8e-b172713cbcaf","metadata":{"language":"python","name":"test_search"},"outputs":[],"source":["myquestion = \"<insert your question here>\"\n","\n","cmd = f\"\"\"\n","    WITH RESULTS \n","    AS (\n","        SELECT \n","            RELATIVE_PATH,\n","            VECTOR_COSINE_DISTANCE(TEXT_CHUNK_VECTOR, \n","                    SNOWFLAKE.ML.EMBED_TEXT('E5-BASE-V2','{myquestion}')) AS DISTANCE,\n","            CHUNK\n","        FROM {chunk_table}\n","        ORDER BY DISTANCE DESC\n","        LIMIT 1\n","    )\n","    SELECT CHUNK, RELATIVE_PATH FROM RESULTS\n","    \"\"\"\n","\n","df_context = session.sql(cmd).to_pandas()\n","prompt_context = df_context._get_value(0,'CHUNK')\n","relative_path =  df_context._get_value(0,'RELATIVE_PATH')\n","print (relative_path)\n","\n","cmd2 = f\"select GET_PRESIGNED_URL(@{docs_stage}, '{relative_path}', 360) as URL_LINK from directory(@{docs_stage})\"\n","df_url_link = session.sql(cmd2).to_pandas()\n","url_link = df_url_link._get_value(0,'URL_LINK')\n","\n","print (url_link)"]},{"cell_type":"markdown","id":"947cae05-1002-49c3-8423-03f4e880fc48","metadata":{"collapsed":false,"name":"CELL9"},"source":["And finally a task is created so when that stream has data the new documents will be processed reading them and creating chunks."]},{"cell_type":"code","execution_count":null,"id":"57931a50","metadata":{},"outputs":[],"source":["session.sql(f\"\"\" \n","    CREATE OR REPLACE TASK {task_name} \n","    WAREHOUSE = DEMO_WH\n","    SCHEDULE = '1 MINUTE'\n","    WHEN SYSTEM$STREAM_HAS_DATA('{docs_stream}')\n","    AS\n","    INSERT INTO {chunk_table} (RELATIVE_PATH, FILE_URL, SCOPED_FILE_URL, CHUNK, TEXT_CHUNK_VECTOR)\n","    SELECT RELATIVE_PATH, \n","            FILE_URL, \n","            BUILD_SCOPED_FILE_URL(@{docs_stage}, RELATIVE_PATH) AS SCOPED_FILE_URL,\n","            FUNC.CHUNK AS CHUNK,\n","            SNOWFLAKE.ML.EMBED_TEXT('e5-base-v2',CHUNK) AS CHUNK_VEC\n","    FROM \n","        {docs_stream},\n","        TABLE(PDF_TEXT_CHUNKER(BUILD_SCOPED_FILE_URL(@{docs_stage}, RELATIVE_PATH))) AS FUNC\n","        WHERE METADATA$ACTION = 'INSERT'\n","\"\"\"\n",").collect()"]},{"cell_type":"code","execution_count":null,"id":"f76bf30a","metadata":{},"outputs":[],"source":["session.sql(f\"ALTER TASK {task_name} RESUME\").collect()"]},{"cell_type":"markdown","id":"1e4f05a2","metadata":{},"source":["Note! If you're running this from inside Snowflake Notebooks, please ensure to upload the Streamlit Python script for this app 'sis_app.py' and it's environment file 'sis_app_environment.yml' to the designated stage before executing the cell after this!"]},{"cell_type":"code","execution_count":null,"id":"721f0ab2","metadata":{},"outputs":[],"source":["session.sql(f\"\"\"\n","            CREATE OR REPLACE STAGE {app_stage} \n","            ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n","            DIRECTORY = (ENABLE = TRUE)\"\"\"\n","        ).collect()\n","\n","if not from_snowflake_notebooks:\n","    session.sql(f\"PUT file://./sis_app.py @{app_stage} overwrite=True auto_compress=False\").collect()\n","    session.sql(f\"PUT file://./sis_app_environment.yml @{app_stage} overwrite=True auto_compress=False\").collect()"]},{"cell_type":"code","execution_count":null,"id":"44aa1001","metadata":{},"outputs":[],"source":["default_wh = session.get_current_warehouse()\n","\n","session.sql(f\"\"\" \n","                CREATE OR REPLACE STREAMLIT pdf_insight\n","                ROOT_LOCATION = '@{app_stage}'\n","                MAIN_FILE = '/sis_app.py'\n","                QUERY_WAREHOUSE = {default_wh}\n","        \"\"\").collect()"]}],"metadata":{"kernelspec":{"display_name":"pdf_insight","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":5}
