{"metadata":{"kernelspec":{"display_name":"Streamlit Notebook","name":"streamlit"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","id":"3775908f-ca36-4846-8f38-5adca39217f2","metadata":{"language":"python","name":"Connect"},"source":"#  Copyright (c) 2023 Snowflake Computing Inc. All rights reserved.\n\n# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"9960b570-1bb7-4a38-a4fe-6927337dba45","metadata":{"name":"CELL1","collapsed":false},"source":"# Demo: Asking Questions to Your Own Documents Using Snowflake Cortex\nThis notebook demo the usage of new Cortex-LLM functions. Using UDTFs (User Defined Table Functions), PDF documents are read and chunked. Embeddings are used to create vectors for each chunk. Those vectors are used later to find similarities with the questions. The user can decide if use RAG to answer questions, so those chunks are provided as context in the prompt.\n\nStreams and Tasks are also used. Each time a new PDF is uploaded into the stage area, it is automatically processed and embeddings are created.\n\nYou can run this notebook and upload your own documents. There is another code with a Streamlit App. This notebook needs to be executed first as it created some of the functions used by the App.\n\nThis notebook assume you have created a staging area called DOCS and have copied some PDFs there."},{"cell_type":"code","id":"51e17718-2982-4114-9c7f-a85be3c97bec","metadata":{"language":"python","name":"list_current_pdfs"},"outputs":[],"source":"docs_available = session.sql(\"ls @docs\").collect()\nlist_docs = []\nfor doc in docs_available:\n    list_docs.append(doc[\"name\"])\nst.dataframe(list_docs)\n","execution_count":null},{"cell_type":"markdown","id":"0e60d590-a73d-4054-97fc-d6bcd16971b7","metadata":{"name":"CELL2","collapsed":false},"source":"We are going to create a function using libraries available in the Snowflake Anaconda channel to read the PDF and chunk it on pieces. This will be registered as a UDTF:"},{"cell_type":"code","id":"d33198fa-7e53-41e8-9b49-47e19ad282f5","metadata":{"language":"python","name":"functions_to_read_chunk"},"outputs":[],"source":"#A class for chunking text and returning a table via UDTF\nfrom snowflake.snowpark.types import StringType, StructField, StructType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom snowflake.snowpark.files import SnowflakeFile\nimport PyPDF2, io\nimport logging\n\nclass pdf_text_chunker:\n\n    def read_pdf(self, file_url: str) -> str:\n    \n        logger = logging.getLogger(\"udf_logger\")\n        logger.info(f\"Opening file {file_url}\")\n    \n        with SnowflakeFile.open(file_url, 'rb') as f:\n            buffer = io.BytesIO(f.readall())\n            \n        reader = PyPDF2.PdfReader(buffer)   \n        text = \"\"\n        for page in reader.pages:\n            try:\n                text += page.extract_text().replace('\\n', ' ')\n            except:\n                text = \"Unable to Extract\"\n                logger.warn(f\"Unable to extract from file {file_url}, page {page}\")\n        \n        return text\n\n    def process(self,file_url: str):\n\n        text = self.read_pdf(file_url)\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 3500, #Adjust this as you see fit\n            chunk_overlap  = 200, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len\n        )\n    \n        chunks = text_splitter.split_text(text)\n        df = pd.DataFrame(chunks, columns=['chunks'])\n        \n        yield from df.itertuples(index=False, name=None)","execution_count":null},{"cell_type":"code","id":"6d0958c8-62a4-46d0-8b56-a96dd718d1ba","metadata":{"language":"python","name":"register_udf","codeCollapsed":false},"outputs":[],"source":"schema = StructType([\n     StructField(\"chunk\", StringType())\n ])\n\nsession.udtf.register( \n    pdf_text_chunker,\n    output_schema= schema, \n    input_types = [StringType()] , \n    is_permanent = True , \n    name = 'pdf_text_chunker' , \n    replace = True , \n    packages=['snowflake-snowpark-python', 'pypdf2','pandas','langchain'], \n    stage_location = 'CC_DOCUMENTS.DATA2.UDF'\n)","execution_count":null},{"cell_type":"markdown","id":"69d752aa-29db-4e1f-9abc-5d3b5ff537fc","metadata":{"name":"CELL3","collapsed":false},"source":"Process the current PDFs in the DOCS staging area. We are using an internal staging area for simplicity but this would be an S3 in the CSP.\n\nHere we call the UDTF defined before to process the documents and create the new table:"},{"cell_type":"code","id":"5769d7f6-dc1f-4ca2-bf34-976c7cdf5073","metadata":{"language":"sql","name":"process_documents"},"outputs":[],"source":"create or replace table docs_chunks_table as\n    select relative_path, \n            size,\n            file_url, \n            build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n            func.chunk as chunk,\n            snowflake.ml.embed_text('e5-base-v2',chunk) as chunk_vec\n    from \n        directory(@docs),\n        TABLE(pdf_text_chunker(build_scoped_file_url(@docs, relative_path))) as func;\n","execution_count":null},{"cell_type":"code","id":"0edda626-0c8f-4a7c-a905-35821d9797b1","metadata":{"language":"sql","name":"check_output"},"outputs":[],"source":"-- To check the column chunk_vec use traditional UI as of today there is an error displaying vector data type here\nselect relative_path, size, file_url, chunk, len(chunk) from docs_chunks_table limit 4;","execution_count":null},{"cell_type":"markdown","id":"9cb47487-5485-4996-863f-f77bb3d7009b","metadata":{"name":"CELL4","collapsed":false},"source":"LetÂ´s do some testing to check it finds documents related to the question we are asking:"},{"cell_type":"code","id":"d45bc2f0-8379-4f50-af8e-b172713cbcaf","metadata":{"language":"python","name":"test_search"},"outputs":[],"source":"myquestion = \"'Write 3 best practices for devops'\"\n\ncmd = f\"\"\"\n    with results as\n    (SELECT RELATIVE_PATH,\n       VECTOR_COSINE_DISTANCE(chunk_vec, \n                snowflake.ml.embed_text('e5-base-v2',{myquestion})) as distance,\n       chunk\n    from docs_chunks_table\n    order by distance desc\n    limit 1)\n    select chunk, relative_path from results\n    \n    \"\"\"\n\ndf_context = session.sql(cmd).to_pandas()\nprompt_context = df_context._get_value(0,'CHUNK')\nrelative_path =  df_context._get_value(0,'RELATIVE_PATH')\nprint (relative_path)\n\ncmd2 = f\"select GET_PRESIGNED_URL(@docs, '{relative_path}', 360) as URL_LINK from directory(@docs)\"\ndf_url_link = session.sql(cmd2).to_pandas()\nurl_link = df_url_link._get_value(0,'URL_LINK')\n\nprint (url_link)","execution_count":null},{"cell_type":"markdown","id":"7ee617e9-c8e3-4d3f-a12d-e8268c1f99ab","metadata":{"name":"CELL5","collapsed":false},"source":"This next function creates the prompt with the question we are asking. If the client wants to use RAG then VECTOR_COSINE_DISTANCE() will be used to find similar chunks to the question being asked and that text will be added to the prompt as context. \n\nOnce the prompt has been built, the cortex function complete() is called to genreate the answer to the question:"},{"cell_type":"code","id":"156295df-c121-421c-bd7a-291b01f26f05","metadata":{"language":"python","name":"functions_for_prompting_complete"},"outputs":[],"source":"\ndef create_prompt (myquestion, rag):\n\n    if rag == 1:\n        myquestion_quoted = f\"'{myquestion}'\"\n    \n        cmd = f\"\"\"\n        with results as\n        (SELECT RELATIVE_PATH,\n           VECTOR_COSINE_DISTANCE(chunk_vec, \n                    snowflake.ml.embed_text('e5-base-v2',{myquestion_quoted})) as distance,\n           chunk\n        from docs_chunks_table\n        order by distance desc\n        limit 1)\n        select chunk, relative_path from results\n        \n        \"\"\"\n        df_context = session.sql(cmd).to_pandas()\n        prompt_context = df_context._get_value(0,'CHUNK')\n        prompt_context = prompt_context.replace(\"'\", \"\")\n        relative_path =  df_context._get_value(0,'RELATIVE_PATH')\n    \n        prompt = f\"\"\"\n         'Answer the question based on the context. Be considse\n          Context: {prompt_context}\n          Question:  \n           {myquestion} \n           Answer: '\n           \"\"\"\n        cmd2 = f\"select GET_PRESIGNED_URL(@docs, '{relative_path}', 360) as URL_LINK from directory(@docs)\"\n        df_url_link = session.sql(cmd2).to_pandas()\n        url_link = df_url_link._get_value(0,'URL_LINK')\n\n    else:\n        prompt = f\"\"\"\n         'Question:  \n           {myquestion} \n           Answer: '\n           \"\"\"\n        url_link = \"None\"\n        relative_path = \"None\"\n        \n    return prompt, url_link, relative_path\n\ndef complete(myquestion, rag = 1):\n\n    prompt, url_link, relative_path =create_prompt (myquestion, rag)\n    cmd = f\"\"\"\n        select snowflake.ml.complete(\n            'llama2-7b-chat',\n            {prompt})\n            as response\n            \"\"\"\n    \n    df_response = session.sql(cmd).collect()\n    return df_response, url_link, relative_path","execution_count":null},{"cell_type":"markdown","id":"07386b87-1fc5-4f98-8089-cf9f6d0f8335","metadata":{"name":"CELL6","collapsed":false},"source":"This helper prints the response and the link to the document used if RAG has been used:"},{"cell_type":"code","id":"b58bf3f4-2d4d-4f50-80cc-6b258f486edd","metadata":{"language":"python","name":"helper_display"},"outputs":[],"source":"def display_response (question, rag=0):\n    response, url_link, relative_path = complete(question, rag)\n    st.markdown(response[0].RESPONSE)\n    if rag == 1:\n        display_url = f\"Link to [{relative_path}]({url_link}) that may be useful\"\n        st.markdown(display_url)\n\n    ","execution_count":null},{"cell_type":"code","id":"9a6467dc-6e68-40be-89df-bb9396c91ccc","metadata":{"language":"python","name":"fill_question"},"outputs":[],"source":"question = \"Write 5 best practices to migrate magento\"","execution_count":null},{"cell_type":"markdown","id":"395b4626-3e0a-441e-91eb-339d2bf2ec2c","metadata":{"name":"CELL7","collapsed":false},"source":"Check the differences when using RAG or not:"},{"cell_type":"code","id":"1a466552-7f5e-41a3-8cd1-fca6228750cb","metadata":{"language":"python","name":"display_no_rag"},"outputs":[],"source":"display_response(question, rag=0)\n\n","execution_count":null},{"cell_type":"code","id":"1ab76d5b-a1cf-4a11-8e74-3fd71a857485","metadata":{"language":"python","name":"display_rag"},"outputs":[],"source":"display_response(question, rag=1)\n\n","execution_count":null},{"cell_type":"markdown","id":"2c02fbc9-5c4b-4751-b4ba-e93bd230658a","metadata":{"name":"CELL8","collapsed":false},"source":"Now we create a stream on the DOCS staging area. This stream will track all new documents uploaded into that staging area."},{"cell_type":"code","id":"bda28d3e-2855-4f8b-9efe-aa425b7e6d14","metadata":{"language":"sql","name":"create_stream"},"outputs":[],"source":"create or replace stream docs_stream on stage docs;\n","execution_count":null},{"cell_type":"markdown","id":"947cae05-1002-49c3-8423-03f4e880fc48","metadata":{"name":"CELL9","collapsed":false},"source":"And finally a task is created so when that stream has data the new documents will be processed reading them and creating chunks."},{"cell_type":"code","id":"8c1cb728-79b5-4ae3-87bb-7a69fb905eae","metadata":{"language":"sql","name":"task_process_stream"},"outputs":[],"source":"create or replace task task_extract_chunk_vec_from_pdf \n    warehouse = XS_WH\n    schedule = '1 minute'\n    when system$stream_has_data('docs_stream')\n    as\n\n    insert into docs_chunks_table (relative_path, size, file_url,\n                            scoped_file_url, chunk, chunk_vec)\n    select relative_path, \n            size,\n            file_url, \n            build_scoped_file_url(@docs, relative_path) as scoped_file_url,\n            func.chunk as chunk,\n            snowflake.ml.embed_text('e5-base-v2',chunk) as chunk_vec\n    from \n        docs_stream,\n        TABLE(pdf_text_chunker(build_scoped_file_url(@docs, relative_path)))            as func;\n","execution_count":null},{"cell_type":"code","id":"c263d4e5-a482-4fd2-89e2-fa890e408446","metadata":{"language":"sql","name":"CELL0"},"outputs":[],"source":"alter task task_extract_chunk_vec_from_pdf resume;","execution_count":null}]}